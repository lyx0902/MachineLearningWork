import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler

# å®šä¹‰ sigmoid å‡½æ•°
def sigmoid(z):
    return 1 / (1 + np.exp(-z))

# æŸå¤±å‡½æ•° (äº¤å‰ç†µ)
def compute_cost(X, y, theta):
    m = len(y)
    h = sigmoid(X @ theta)
    cost = -(1 / m) * (y @ np.log(h) + (1 - y) @ np.log(1 - h))
    return cost

# æ¢¯åº¦ä¸‹é™å®ç° Logistic å›å½’
def logistic_regression_gd(X, y, alpha=0.01, epochs=50):
    m, n = X.shape
    theta = np.zeros(n)
    cost_history = []
    for epoch in range(epochs):
        z = X @ theta
        h = sigmoid(z)
        gradient = (1 / m) * X.T @ (h - y)
        theta -= alpha * gradient
        cost_history.append(compute_cost(X, y, theta))
    return theta, cost_history

# éšæœºæ¢¯åº¦ä¸‹é™å®ç° Logistic å›å½’
def logistic_regression_sgd(X, y, alpha=0.01, epochs=50):
    m, n = X.shape
    theta = np.zeros(n)
    cost_history = []
    for epoch in range(epochs):
        for i in range(m):
            xi = X[i, :].reshape(1, -1)
            yi = y[i]
            z = xi @ theta
            h = sigmoid(z)
            gradient = (h - yi) * xi
            theta -= alpha * gradient.flatten()
        cost_history.append(compute_cost(X, y, theta))
    return theta, cost_history

# ç‰›é¡¿æ³•å®ç° Logistic å›å½’
def logistic_regression_newton(X, y, epochs=50):
    m, n = X.shape
    theta = np.zeros(n)
    cost_history = []
    for epoch in range(epochs):
        z = X @ theta
        h = sigmoid(z)
        gradient = (1 / m) * X.T @ (h - y)
        H = (1 / m) * X.T @ np.diag(h * (1 - h)) @ X  # Hessian çŸ©é˜µ
        theta -= np.linalg.inv(H) @ gradient
        cost_history.append(compute_cost(X, y, theta))
    return theta, cost_history

# æ•°æ®é¢„å¤„ç†
def load_and_preprocess_data():
    # åŠ è½½æ•°æ®
    ex4x = np.loadtxt('dataset/ex4x.dat')
    ex4y = np.loadtxt('dataset/ex4y.dat')

    # æ ‡å‡†åŒ–æ•°æ®
    scaler = StandardScaler()
    ex4x_normalized = scaler.fit_transform(ex4x)

    # æ·»åŠ åç½®é¡¹
    X = np.c_[np.ones((ex4x_normalized.shape[0], 1)), ex4x_normalized]
    y = ex4y.astype(int)

    return X, y

# ç»˜åˆ¶æŸå¤±å˜åŒ–æ›²çº¿
def plot_cost_history(cost_history, method):
    plt.plot(cost_history, label=method)
    plt.xlabel('Iterations')
    plt.ylabel('Cost')
    plt.title(f'Cost vs. Iterations ({method})')
    plt.legend()

# ç»˜åˆ¶å†³ç­–è¾¹ç•Œ
def plot_decision_boundary(X, y, theta, method):
    plt.figure(figsize=(8, 6))
    plt.scatter(X[y == 0, 1], X[y == 0, 2], label="Not Admitted", marker='o', color='blue')
    plt.scatter(X[y == 1, 1], X[y == 1, 2], label="Admitted", marker='+', color='red')

    # å†³ç­–è¾¹ç•Œ: ğœƒâ‚€ + ğœƒâ‚xâ‚ + ğœƒâ‚‚xâ‚‚ = 0 -> xâ‚‚ = -(ğœƒâ‚€ + ğœƒâ‚xâ‚) / ğœƒâ‚‚
    x_values = np.linspace(X[:, 1].min(), X[:, 1].max(), 100)
    y_values = -(theta[0] + theta[1] * x_values) / theta[2]
    plt.plot(x_values, y_values, label='Decision Boundary', color='green')

    plt.xlabel('Exam 1 Score (Standardized)')
    plt.ylabel('Exam 2 Score (Standardized)')
    plt.title(f'Decision Boundary ({method})')
    plt.legend()

# ä¸»ç¨‹åº
if __name__ == "__main__":
    X, y = load_and_preprocess_data()

    # æ¢¯åº¦ä¸‹é™
    theta_gd, cost_history_gd = logistic_regression_gd(X, y, alpha=0.1, epochs=50)

    # éšæœºæ¢¯åº¦ä¸‹é™
    theta_sgd, cost_history_sgd = logistic_regression_sgd(X, y, alpha=0.1, epochs=50)

    # ç‰›é¡¿æ³•
    theta_newton, cost_history_newton = logistic_regression_newton(X, y, epochs=50)

    # æ‰“å°æœ€ç»ˆç»“æœ
    print("Logistic å›å½’ (æ¢¯åº¦ä¸‹é™) å‚æ•°:", theta_gd)
    print("Logistic å›å½’ (éšæœºæ¢¯åº¦ä¸‹é™) å‚æ•°:", theta_sgd)
    print("Logistic å›å½’ (ç‰›é¡¿æ³•) å‚æ•°:", theta_newton)

    # ç»˜åˆ¶æŸå¤±å‡½æ•°æ›²çº¿
    plt.figure(figsize=(8, 6))
    plot_cost_history(cost_history_gd, "Gradient Descent")
    plt.savefig("T2Graphs/cost_gd.png")
    plt.show()

    plt.figure(figsize=(8, 6))
    plot_cost_history(cost_history_sgd, "Stochastic Gradient Descent")
    plt.savefig("T2Graphs/cost_sgd.png")
    plt.show()

    plt.figure(figsize=(8, 6))
    plot_cost_history(cost_history_newton, "Newton's Method")
    plt.savefig("T2Graphs/cost_newton.png")
    plt.show()

    # ç»˜åˆ¶å†³ç­–è¾¹ç•Œ
    plot_decision_boundary(X, y, theta_gd, "Gradient Descent")
    plt.savefig("T2Graphs/boundary_gd.png")
    plt.show()

    plot_decision_boundary(X, y, theta_sgd, "Stochastic Gradient Descent")
    plt.savefig("T2Graphs/boundary_sgd.png")
    plt.show()

    plot_decision_boundary(X, y, theta_newton, "Newton's Method")
    plt.savefig("T2Graphs/boundary_newton.png")
    plt.show()
